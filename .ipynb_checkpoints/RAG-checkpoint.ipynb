{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11d49bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yin/anaconda3/envs/env_llm/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser, PydanticOutputParser\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea7e45",
   "metadata": {},
   "source": [
    "# Load documents and upload to vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890cf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "os.environ['PINECONE_API_KEY'] = '27ba301a-0ec5-4c84-890f-39bd6c5b1ff0'\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8d08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Docuemnt loader \n",
    "\n",
    "loader = DirectoryLoader('./data/', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2 : Text splitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e9f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example content:\n",
      "\n",
      "mans watching complex image or video stimuli (e.g. [20],\n",
      "[21]). New databases have emerged by following two trends,\n",
      "1) increasing the number of images, and 2) introducing new\n",
      "measurements to saliency by providing contextual annota-\n",
      "tions (e.g. image categories, regional properties, etc.). To an-\n",
      "notate large scale data, researchers have resorted to crowd-\n",
      "sourcing schemes such as gaze tracking using webcams [19]\n",
      "or mouse movements [22], [23] as alternatives to lab-based\n",
      "eye trackers (Fig. 3). Deep supervised saliency models rely\n",
      "heavily on these sufﬁciently large and well-labeled datasets.\n",
      "Here, I review some of the most recent and inﬂuential image\n",
      "and video datasets. The discussion of pros and cons of these\n",
      "datasets is postponed to Section 4. For a review of ﬁxation\n",
      "datasets pre-deep learning era please consult [24].\n",
      "Image datasets. Three of the most popular image datasets\n",
      "used for training and testing models are as follows.\n",
      "• MIT300: This dataset is a collection of 300 natural\n",
      "\n",
      "Example metadata:\n",
      "\n",
      "{'source': 'data/Saliency Prediction in Deep Learning Era.pdf', 'file_path': 'data/Saliency Prediction in Deep Learning Era.pdf', 'page': 1, 'total_pages': 44, 'format': 'PDF 1.6', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20190528001822Z', 'modDate': \"D:20230410223806+08'00'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example content:\\n\")\n",
    "print(splits[10].page_content)\n",
    "\n",
    "print(\"\\nExample metadata:\\n\")\n",
    "\n",
    "print(splits[10].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a65a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yin/anaconda3/envs/env_llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/yin/anaconda3/envs/env_llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Embedding Model\n",
    "# More info on model: https://huggingface.co/BAAI/bge-small-en\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ca5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"research-paper-index\"\n",
    "\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "if index_name not in existing_indexes:\n",
    "\n",
    "    pc.create_index(\n",
    "      name=index_name,\n",
    "      dimension=384, # based on model dimensions\n",
    "      metric=\"cosine\",\n",
    "      spec=ServerlessSpec(\n",
    "          cloud='aws', \n",
    "          region='us-east-1'\n",
    "      ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca896e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 283}},\n",
       " 'total_vector_count': 283}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert data to vector db\n",
    "docsearch = PineconeVectorStore.from_documents(splits, embedding_function, index_name=index_name)\n",
    "\n",
    "# view index stats\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "\n",
    "# docsearch.add_texts([\"More text!\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee7dc8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47064665",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain mask r-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m docsearch\u001b[38;5;241m.\u001b[39msimilarity_search(query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docsearch' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"Explain mask r-cnn\"\n",
    "docs = docsearch.similarity_search(query, k=2)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee507725",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Document 0\n",
      "\n",
      "3. Mask R-CNN\n",
      "Mask R-CNN is conceptually simple: Faster R-CNN has\n",
      "two outputs for each candidate object, a class label and a\n",
      "bounding-box offset; to this we add a third branch that out-\n",
      "puts the object mask. Mask R-CNN is thus a natural and in-\n",
      "tuitive idea. But the additional mask output is distinct from\n",
      "the class and box outputs, requiring extraction of much ﬁner\n",
      "spatial layout of an object. Next, we introduce the key ele-\n",
      "ments of Mask R-CNN, including pixel-to-pixel alignment,\n",
      "which is the main missing piece of Fast/Faster R-CNN.\n",
      "Faster R-CNN: We begin by brieﬂy reviewing the Faster\n",
      "R-CNN detector [29]. Faster R-CNN consists of two stages.\n",
      "The ﬁrst stage, called a Region Proposal Network (RPN),\n",
      "proposes candidate object bounding boxes.\n",
      "The second\n",
      "stage, which is in essence Fast R-CNN [9], extracts features\n",
      "using RoIPool from each candidate box and performs clas-\n",
      "siﬁcation and bounding-box regression. The features used\n",
      "by both stages can be shared for faster inference. We re-\n",
      "\n",
      "## Document 1\n",
      "\n",
      "Mask R-CNN\n",
      "Kaiming He\n",
      "Georgia Gkioxari\n",
      "Piotr Doll´\n",
      "ar\n",
      "Ross Girshick\n",
      "Facebook AI Research (FAIR)\n",
      "Abstract\n",
      "We present a conceptually simple, ﬂexible, and general\n",
      "framework for object instance segmentation. Our approach\n",
      "efﬁciently detects objects in an image while simultaneously\n",
      "generating a high-quality segmentation mask for each in-\n",
      "stance. The method, called Mask R-CNN, extends Faster\n",
      "R-CNN by adding a branch for predicting an object mask in\n",
      "parallel with the existing branch for bounding box recogni-\n",
      "tion. Mask R-CNN is simple to train and adds only a small\n",
      "overhead to Faster R-CNN, running at 5 fps. Moreover,\n",
      "Mask R-CNN is easy to generalize to other tasks, e.g., al-\n",
      "lowing us to estimate human poses in the same framework.\n",
      "We show top results in all three tracks of the COCO suite of\n",
      "challenges, including instance segmentation, bounding-box\n",
      "object detection, and person keypoint detection. Without\n",
      "tricks, Mask R-CNN outperforms all existing, single-model\n"
     ]
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "matched_docs = retriever.invoke(query)\n",
    "for i, d in enumerate(matched_docs):\n",
    "    print(f\"\\n## Document {i}\\n\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5c875",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41efde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"research-paper-index\"\n",
    "\n",
    "docsearch = PineconeVectorStore(index_name=index_name, embedding=embedding_function)\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd9b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "chat_model = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.001,\n",
    "        \"return_full_text\" : False\n",
    "    },\n",
    ")\n",
    "\n",
    "# load a chat model\n",
    "\n",
    "# chat_model = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"return_full_text\" : False\n",
    "#     },\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c04e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(description=\"question asked by user\")\n",
    "    answer: str = Field(description=\"answer from model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25795061",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
    "format_instructions = parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c5b746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_schema = ResponseSchema(name=\"question\", description=\"user's question\")\n",
    "answer_schema = ResponseSchema(name=\"answer\", description=\"answer from model\")\n",
    "\n",
    "response_schemas = [question_schema, answer_schema]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1b7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_template_without_context = \"\"\" Answer the question based on your understanding. \n",
    "Keep the answer short and concise. \n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_template_with_context = \"\"\" Answer the question based on the context below. \n",
    "Keep the answer short and concise. \n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_prompt_without_context = PromptTemplate.from_template(template=rag_template_without_context,\n",
    "                                partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "rag_prompt_with_context = PromptTemplate.from_template(template=rag_template_with_context,\n",
    "                                partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "        \n",
    "    \n",
    "rag_chain_without_context = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_without_context\n",
    "    | chat_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "rag_chain_with_context = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_with_context\n",
    "    | chat_model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb2e702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bdd2f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without own data: \n",
      "\n",
      "{'question': 'What are the categories of attentional models', 'answer': 'The categories of attentional models include Saliency-based models, Object-based models, and Feature-based models.'}\n"
     ]
    }
   ],
   "source": [
    "response_without_context = rag_chain_without_context.invoke(\"What are the categories of attentional models\")\n",
    "\n",
    "print(\"Response without own data: \\n\")\n",
    "print(response_without_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03170488",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with own data: \n",
      "\n",
      "{'question': 'What are the categories of attentional models', 'answer': 'task-agnostic approaches (i.e. finding the salient pieces of information, a.k.a bottom-up (BU) saliency [1]–[4]) and task-specific methods (i.e. finding'}\n"
     ]
    }
   ],
   "source": [
    "response_with_context = rag_chain_with_context.invoke(\"What are the categories of attentional models\")\n",
    "\n",
    "print(\"Response with own data: \\n\")\n",
    "print(response_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169b88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
