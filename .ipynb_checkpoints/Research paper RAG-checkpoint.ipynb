{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f03818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57116405",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load own documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b287d17",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf1143b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load own documents from directory\n",
    "\n",
    "loader = DirectoryLoader('./data/', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "# break down documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c4e698",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example content:\n",
      "\n",
      "techniques and main modeling trends, and iii) datasets\n",
      "and evaluation metrics in salient object detection.\n",
      "We\n",
      "also discuss open problems such as evaluation metrics\n",
      "and dataset bias in model performance and suggest future\n",
      "research directions.\n",
      "Keywords Salient object detection, bottom-up saliency,\n",
      "explicit saliency, visual attention, regions of\n",
      "interest.\n",
      "1\n",
      "Introduction\n",
      "Humans are able to detect visually distinctive, so called\n",
      "salient, scene regions effortlessly and rapidly (i.e.,\n",
      "pre-\n",
      "attentive stage). These ﬁltered regions are then perceived\n",
      "and processed in ﬁner details for the extraction of richer\n",
      "high-level information (i.e., attentive stage). This capability\n",
      "has long been studied by cognitive scientists and has\n",
      "recently attracted a lot of interest in the computer vision\n",
      "community mainly because it helps ﬁnd the objects or\n",
      "1\n",
      "MarkableAI. E-mail: ali@markable.ai.\n",
      "2\n",
      "TKLNDST, College of Computer Science, Nankai University.\n",
      "E-mail: cmm@nankai.edu.cn\n",
      "3\n",
      "University of Massachusetts Amherst.\n",
      "\n",
      "Example metadata:\n",
      "\n",
      "{'source': 'data/Salient Object Detection A Survey.pdf', 'file_path': 'data/Salient Object Detection A Survey.pdf', 'page': 0, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20190702012310Z', 'modDate': 'D:20190702012310Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example content:\\n\")\n",
    "print(splits[1].page_content)\n",
    "\n",
    "print(\"\\nExample metadata:\\n\")\n",
    "\n",
    "print(splits[1].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa10ce",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Upload to vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbc6c37",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yin/anaconda3/envs/env_llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/yin/anaconda3/envs/env_llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model to generate embeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e605f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the vector db / index\n",
    "\n",
    "index_name = \"research-paper-index\"\n",
    "\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "if index_name not in existing_indexes:\n",
    "\n",
    "    pc.create_index(\n",
    "      name=index_name,\n",
    "      dimension=384, # based on model output dimensions\n",
    "      metric=\"cosine\",\n",
    "      spec=ServerlessSpec(\n",
    "          cloud='aws', \n",
    "          region='us-east-1'\n",
    "      ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d695e01a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 403}},\n",
       " 'total_vector_count': 403}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert data to vector db\n",
    "docsearch = PineconeVectorStore.from_documents(splits, embedding_function, index_name=index_name)\n",
    "\n",
    "# view index stats\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "\n",
    "# docsearch.add_texts([\"More text!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b2b3a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total_vector_count should be same as amounts of chunks\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8f7cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Retrieve similar data given a query\n",
    "\n",
    "Perform similarity search to find relevant info in vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94122dbc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To separate these two types of saliency models, in this\n",
      "study we provide a precise deﬁnition and suggest an ap-\n",
      "propriate treatment of salient object detection. Generally, a\n",
      "salient object detection model should, ﬁrst detect the salient\n",
      "attention-grabbing objects in a scene, and second, segment\n",
      "the entire objects. Usually, the output of the model is a\n",
      "saliency map where the intensity of each pixel represents\n",
      "its probability of belonging to salient objects. From this\n",
      "deﬁnition, we can see that this problem in its essence\n",
      "is a ﬁgure/ground segmentation problem, and the goal is\n",
      "to only segment the salient foreground object from the\n",
      "background. Note that it slightly differs from the traditional\n",
      "image segmentation problem that aims to partition an image\n",
      "into perceptually coherent regions.\n",
      "The value of salient object detection models lies in their\n",
      "applications in many areas such as computer vision, graph-\n",
      "ics, and robotics. For instance, these models have been suc-\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain saliency detection\"\n",
    "docs = docsearch.similarity_search(query, k=1)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af343a77",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Document 0\n",
      "\n",
      "To separate these two types of saliency models, in this\n",
      "study we provide a precise deﬁnition and suggest an ap-\n",
      "propriate treatment of salient object detection. Generally, a\n",
      "salient object detection model should, ﬁrst detect the salient\n",
      "attention-grabbing objects in a scene, and second, segment\n",
      "the entire objects. Usually, the output of the model is a\n",
      "saliency map where the intensity of each pixel represents\n",
      "its probability of belonging to salient objects. From this\n",
      "deﬁnition, we can see that this problem in its essence\n",
      "is a ﬁgure/ground segmentation problem, and the goal is\n",
      "to only segment the salient foreground object from the\n",
      "background. Note that it slightly differs from the traditional\n",
      "image segmentation problem that aims to partition an image\n",
      "into perceptually coherent regions.\n",
      "The value of salient object detection models lies in their\n",
      "applications in many areas such as computer vision, graph-\n",
      "ics, and robotics. For instance, these models have been suc-\n"
     ]
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "matched_docs = retriever.invoke(query)\n",
    "for i, d in enumerate(matched_docs):\n",
    "    print(f\"\\n## Document {i}\\n\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e8e4b",
   "metadata": {},
   "source": [
    "# Generate a response\n",
    "\n",
    "The result above is not suitable to be output to user. We will need a chat model to reformat the content above to make it more presentable as final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4be0745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yin/anaconda3/envs/env_llm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# load open-source chat model from Huggingface\n",
    "chat_model = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.001,\n",
    "        \"return_full_text\" : False\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# chat_model = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     task=\"text-generation\",\n",
    "#     model_kwargs={\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"return_full_text\" : False\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8539ee9",
   "metadata": {},
   "source": [
    "## Define the output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5760248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output data structure\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(description=\"question asked by user\")\n",
    "    answer: str = Field(description=\"answer from model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "561ba7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"question asked by user\", \"type\": \"string\"}, \"answer\": {\"title\": \"Answer\", \"description\": \"answer from model\", \"type\": \"string\"}}, \"required\": [\"question\", \"answer\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325ac08",
   "metadata": {},
   "source": [
    "## RAG without own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e1235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the prompt\n",
    "\n",
    "rag_template_without_context = \"\"\" Answer the question based on your understanding. \n",
    "Keep the answer short and concise. \n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_without_context = PromptTemplate.from_template(template=rag_template_without_context,\n",
    "                                partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "rag_chain_without_context = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_without_context\n",
    "    | chat_model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9254061d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<question>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFaceHub] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Answer the question based on your understanding. \\nKeep the answer short and concise. \\nRespond \\\"Unsure about answer\\\" if not sure about the answer.\\n\\nQuestion: What are the categories of attentional models\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"question\\\": {\\\"title\\\": \\\"Question\\\", \\\"description\\\": \\\"question asked by user\\\", \\\"type\\\": \\\"string\\\"}, \\\"answer\\\": {\\\"title\\\": \\\"Answer\\\", \\\"description\\\": \\\"answer from model\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"question\\\", \\\"answer\\\"]}\\n```\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFaceHub] [506ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Here is the output:\\n```\\n{\\n  \\\"question\\\": \\\"What are the categories of attentional models\\\",\\n  \\\"answer\\\": \\\"There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.\\\"\\n}\\n```  Answer: There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.  JSON Output:  {\\\"question\\\": \\\"What are the categories of attentional models\\\", \\\"answer\\\": \\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Here is the output:\\n```\\n{\\n  \\\"question\\\": \\\"What are the categories of attentional models\\\",\\n  \\\"answer\\\": \\\"There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.\\\"\\n}\\n```  Answer: There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.  JSON Output:  {\\\"question\\\": \\\"What are the categories of attentional models\\\", \\\"answer\\\": \\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [13ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the categories of attentional models\",\n",
      "  \"answer\": \"There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [525ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the categories of attentional models\",\n",
      "  \"answer\": \"There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# run the RAG chain\n",
    "response_without_context = rag_chain_without_context.invoke(\"What are the categories of attentional models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14bae346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without own data: \n",
      "\n",
      "{'question': 'What are the categories of attentional models', 'answer': 'There are three categories of attentional models: Sustained Attention, Selective Attention, and Alternating Attention.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Response without own data: \\n\")\n",
    "print(response_without_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71c9ca",
   "metadata": {},
   "source": [
    "## RAG with own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1921885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the prompt\n",
    "\n",
    "rag_template_with_context = \"\"\" Answer the question based on the context below. \n",
    "Keep the answer short and concise. \n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt_with_context = PromptTemplate.from_template(template=rag_template_with_context,\n",
    "                                partial_variables={\"format_instructions\": parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ba739d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66edf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_context_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | rag_prompt_with_context\n",
    "    | chat_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "                                        ).assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66634c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [5.18s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency\\njudgments relate to ﬁxations, how to conduct fair model comparison, and what are the emerging applications of saliency models.\\nIndex Terms—Visual saliency, eye movement prediction, attention, video saliency, benchmark, deep learning.\\n!\\n1\\nINTRODUCTION\\nV\\nIsual attention enables humans to rapidly analyze com-\\nplex scenes and devote their limited perceptual and\\ncognitive resources to the most pertinent subsets of sensory\\ndata. It acts as a shiftable information processing bottleneck,\\nallowing only objects within a circumscribed region to reach\\nhigher levels of processing and visual awareness [1].\\nBroadly speaking, the literature on attentional models\\ncan be split into two categories: task-agnostic approaches\\n(i.e. ﬁnding the salient pieces of information, a.k.a bottom-up\\n(BU) saliency [1]–[4]) and task-speciﬁc methods (i.e. ﬁnding\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency\\njudgments relate to ﬁxations, how to conduct fair model comparison, and what are the emerging applications of saliency models.\\nIndex Terms—Visual saliency, eye movement prediction, attention, video saliency, benchmark, deep learning.\\n!\\n1\\nINTRODUCTION\\nV\\nIsual attention enables humans to rapidly analyze com-\\nplex scenes and devote their limited perceptual and\\ncognitive resources to the most pertinent subsets of sensory\\ndata. It acts as a shiftable information processing bottleneck,\\nallowing only objects within a circumscribed region to reach\\nhigher levels of processing and visual awareness [1].\\nBroadly speaking, the literature on attentional models\\ncan be split into two categories: task-agnostic approaches\\n(i.e. ﬁnding the salient pieces of information, a.k.a bottom-up\\n(BU) saliency [1]–[4]) and task-speciﬁc methods (i.e. ﬁnding\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > chain:RunnableAssign<context>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency\\njudgments relate to ﬁxations, how to conduct fair model comparison, and what are the emerging applications of saliency models.\\nIndex Terms—Visual saliency, eye movement prediction, attention, video saliency, benchmark, deep learning.\\n!\\n1\\nINTRODUCTION\\nV\\nIsual attention enables humans to rapidly analyze com-\\nplex scenes and devote their limited perceptual and\\ncognitive resources to the most pertinent subsets of sensory\\ndata. It acts as a shiftable information processing bottleneck,\\nallowing only objects within a circumscribed region to reach\\nhigher levels of processing and visual awareness [1].\\nBroadly speaking, the literature on attentional models\\ncan be split into two categories: task-agnostic approaches\\n(i.e. ﬁnding the salient pieces of information, a.k.a bottom-up\\n(BU) saliency [1]–[4]) and task-speciﬁc methods (i.e. ﬁnding\",\n",
      "  \"question\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency\\njudgments relate to ﬁxations, how to conduct fair model comparison, and what are the emerging applications of saliency models.\\nIndex Terms—Visual saliency, eye movement prediction, attention, video saliency, benchmark, deep learning.\\n!\\n1\\nINTRODUCTION\\nV\\nIsual attention enables humans to rapidly analyze com-\\nplex scenes and devote their limited perceptual and\\ncognitive resources to the most pertinent subsets of sensory\\ndata. It acts as a shiftable information processing bottleneck,\\nallowing only objects within a circumscribed region to reach\\nhigher levels of processing and visual awareness [1].\\nBroadly speaking, the literature on attentional models\\ncan be split into two categories: task-agnostic approaches\\n(i.e. ﬁnding the salient pieces of information, a.k.a bottom-up\\n(BU) saliency [1]–[4]) and task-speciﬁc methods (i.e. ﬁnding\",\n",
      "  \"question\": \"What are the categories of attentional models\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > llm:HuggingFaceHub] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Answer the question based on the context below. \\nKeep the answer short and concise. \\nRespond \\\"Unsure about answer\\\" if not sure about the answer.\\n\\nContext: what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency\\njudgments relate to ﬁxations, how to conduct fair model comparison, and what are the emerging applications of saliency models.\\nIndex Terms—Visual saliency, eye movement prediction, attention, video saliency, benchmark, deep learning.\\n!\\n1\\nINTRODUCTION\\nV\\nIsual attention enables humans to rapidly analyze com-\\nplex scenes and devote their limited perceptual and\\ncognitive resources to the most pertinent subsets of sensory\\ndata. It acts as a shiftable information processing bottleneck,\\nallowing only objects within a circumscribed region to reach\\nhigher levels of processing and visual awareness [1].\\nBroadly speaking, the literature on attentional models\\ncan be split into two categories: task-agnostic approaches\\n(i.e. ﬁnding the salient pieces of information, a.k.a bottom-up\\n(BU) saliency [1]–[4]) and task-speciﬁc methods (i.e. ﬁnding\\nQuestion: What are the categories of attentional models\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"question\\\": {\\\"title\\\": \\\"Question\\\", \\\"description\\\": \\\"question asked by user\\\", \\\"type\\\": \\\"string\\\"}, \\\"answer\\\": {\\\"title\\\": \\\"Answer\\\", \\\"description\\\": \\\"answer from model\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"question\\\", \\\"answer\\\"]}\\n```\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > llm:HuggingFaceHub] [1.58s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Here is the output for the given question:\\n```\\n{\\n  \\\"question\\\": \\\"What are the categories of attentional models\\\",\\n  \\\"answer\\\": \\\"task-agnostic approaches and task-speciﬁc methods\\\"\\n}\\n```  Answer: {\\\"question\\\": \\\"What are the categories of attentional models\\\", \\\"answer\\\": \\\"task-agnostic approaches and task-speciﬁc methods\\\"}  JSON output: {\\\"question\\\": \\\"What are the categories of attentional models\\\", \\\"answer\\\": \\\"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Here is the output for the given question:\\n```\\n{\\n  \\\"question\\\": \\\"What are the categories of attentional models\\\",\\n  \\\"answer\\\": \\\"task-agnostic approaches and task-speciﬁc methods\\\"\\n}\\n```  Answer: {\\\"question\\\": \\\"What are the categories of attentional models\\\", \\\"answer\\\": \\\"task-agnostic approaches and task-speciﬁc methods\\\"}  JSON output: {\\\"question\\\": \\\"What are the categories of attentional models\\\", \\\"answer\\\": \\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence > parser:JsonOutputParser] [13ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the categories of attentional models\",\n",
      "  \"answer\": \"task-agnostic approaches and task-speciﬁc methods\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:RunnableSequence] [1.60s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are the categories of attentional models\",\n",
      "  \"answer\": \"task-agnostic approaches and task-speciﬁc methods\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] [1.61s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"answer\": {\n",
      "    \"question\": \"What are the categories of attentional models\",\n",
      "    \"answer\": \"task-agnostic approaches and task-speciﬁc methods\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] [1.62s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6.82s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "response_with_context_with_source = rag_chain_with_source.invoke(\"What are the categories of attentional models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe6d72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from db that matched the query:\n",
      "\n",
      "[Document(page_content='what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency\\njudgments relate to ﬁxations, how to conduct fair model comparison, and what are the emerging applications of saliency models.\\nIndex Terms—Visual saliency, eye movement prediction, attention, video saliency, benchmark, deep learning.\\n!\\n1\\nINTRODUCTION\\nV\\nIsual attention enables humans to rapidly analyze com-\\nplex scenes and devote their limited perceptual and\\ncognitive resources to the most pertinent subsets of sensory\\ndata. It acts as a shiftable information processing bottleneck,\\nallowing only objects within a circumscribed region to reach\\nhigher levels of processing and visual awareness [1].\\nBroadly speaking, the literature on attentional models\\ncan be split into two categories: task-agnostic approaches\\n(i.e. ﬁnding the salient pieces of information, a.k.a bottom-up\\n(BU) saliency [1]–[4]) and task-speciﬁc methods (i.e. ﬁnding', metadata={'author': '', 'creationDate': 'D:20190528001822Z', 'creator': 'LaTeX with hyperref package', 'file_path': 'data/Saliency Prediction in Deep Learning Era.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20190528001822Z', 'page': 0.0, 'producer': 'pdfTeX-1.40.17', 'source': 'data/Saliency Prediction in Deep Learning Era.pdf', 'subject': '', 'title': '', 'total_pages': 44.0, 'trapped': ''})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data from db that matched the query:\\n\")\n",
    "print(response_with_context_with_source.get(\"context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0be81925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What are the categories of attentional models', 'answer': 'task-agnostic approaches and task-speciﬁc methods'}\n"
     ]
    }
   ],
   "source": [
    "print(response_with_context_with_source.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7164dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
